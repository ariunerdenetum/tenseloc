{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11436425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sae_lens import HookedSAETransformer, SAE\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import transformer_lens.utils as utils\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8707a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bdefee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x14e3696ca800>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME   = \"meta-llama/Llama-3.1-8B\"\n",
    "HF_TOKEN     = \"...\"\n",
    "FEATURES_CSV = \"./latent_outputs_yusser/chosen_features.csv\"\n",
    "DEV_PROMPTS  = \"./prompts_dataset_dev.json\"\n",
    "TEST_PROMPTS = \"./prompts_dataset_test.json\"\n",
    "LAYERS       = [15, 16]\n",
    "ALPHAS       = [1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "DEVICE       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Thresholds for labeling\n",
    "ACCURACY_THRESHOLD = {\n",
    "    'past': 0.74,\n",
    "    'present': 0.64,\n",
    "    'future': 0.8,\n",
    "}\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d5bab",
   "metadata": {},
   "source": [
    "Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a363be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/ariuka/tense/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e59090e2034908b2b187c4906e9135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    device_map=\"auto\", \n",
    "    use_auth_token=HF_TOKEN\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Custom [PAD] special token\n",
    "if tokenizer.pad_token is None:\n",
    "    # reuse eos_token as pad\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7706362",
   "metadata": {},
   "source": [
    "Load SAEs for each layer and collect condidate feature indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d294b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {15: [30777,\n",
       "              702,\n",
       "              28855,\n",
       "              5112,\n",
       "              23112,\n",
       "              32090,\n",
       "              26492,\n",
       "              12722,\n",
       "              15316,\n",
       "              7890,\n",
       "              15706],\n",
       "             16: [5215,\n",
       "              1221,\n",
       "              32043,\n",
       "              3689,\n",
       "              9951,\n",
       "              6922,\n",
       "              25624,\n",
       "              17716,\n",
       "              7895,\n",
       "              3638,\n",
       "              12508,\n",
       "              28602,\n",
       "              23504],\n",
       "             17: [26956,\n",
       "              30639,\n",
       "              23198,\n",
       "              2164,\n",
       "              9067,\n",
       "              363,\n",
       "              1383,\n",
       "              32205,\n",
       "              6612,\n",
       "              17052,\n",
       "              3212,\n",
       "              10675,\n",
       "              469,\n",
       "              12337]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV of chosen features (with columns: label, layer, feature)\n",
    "df_feats = pd.read_csv(FEATURES_CSV)\n",
    "\n",
    "candidate_features = defaultdict(list)\n",
    "for _, row in df_feats.iterrows():\n",
    "    candidate_features[int(row['layer'])].append(int(row['feature']))\n",
    "    \n",
    "for k, v in candidate_features.items():\n",
    "    candidate_features[k] = list(dict.fromkeys(v))\n",
    "    \n",
    "candidate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449476cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/ariuka/tense/venv/lib/python3.10/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks.15.hook_resid_post', 'blocks.16.hook_resid_post'])\n"
     ]
    }
   ],
   "source": [
    "# Pre-load SAE modules\n",
    "saes = {}\n",
    "for layer in LAYERS:\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    sae_module, _, _ = SAE.from_pretrained(\n",
    "        release=\"Yusser/multilingual_llama3.1-8B_saes\",\n",
    "        sae_id=hook_name,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    sae_module.eval()\n",
    "#     saes[hook_name] = sae_module\n",
    "    saes[sae_module.cfg.hook_name] = sae_module\n",
    "\n",
    "print(saes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8586a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScaledSAE wrapper at module scope\n",
    "torch.set_grad_enabled(False)\n",
    "class ScaledSAE(torch.nn.Module):\n",
    "    def __init__(self, base_sae, idxs, alpha, hook_name):\n",
    "        super().__init__()\n",
    "        self.sae   = base_sae\n",
    "        self.idxs  = idxs\n",
    "        self.alpha = alpha\n",
    "        self.cfg   = base_sae.cfg\n",
    "        self.cfg.hook_name = hook_name\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.sae.encode(z)\n",
    "        if self.idxs:\n",
    "            h[:, :, self.idxs] *= self.alpha\n",
    "        return self.sae.decode(h)\n",
    "    \n",
    "# class ScaledSAE(torch.nn.Module):\n",
    "#     def __init__(self, sae, idxs, alpha, hook_name):\n",
    "#         super().__init__()\n",
    "#         self.sae   = sae\n",
    "#         self.idxs  = idxs\n",
    "#         self.alpha = alpha\n",
    "#         self.cfg   = sae.cfg\n",
    "#         self.cfg.hook_name = hook_name\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         h = self.sae.encode(z)\n",
    "#         h[:, :, self.idxs] *= self.alpha\n",
    "#         return self.sae.decode(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffca9d8",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ac5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_with_saes(layer, feature, alpha, prompts):\n",
    "    counts   = defaultdict(int)\n",
    "    corrects = defaultdict(int)\n",
    "\n",
    "    # Build scaled SAE list: only scale the given feature at given layer\n",
    "    scaled_saes = []\n",
    "    for lyr in LAYERS:\n",
    "        hook_name = utils.get_act_name(\"resid_post\", lyr)\n",
    "        base_sae  = saes[hook_name]\n",
    "        if lyr == layer:\n",
    "            idxs = [feature]\n",
    "            alpha_val = alpha\n",
    "        else:\n",
    "            idxs = []\n",
    "            alpha_val = 1.0\n",
    "        scaled_saes.append(ScaledSAE(base_sae, idxs, alpha_val, hook_name))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ex in prompts:\n",
    "            prompt = ex[\"prompt_text\"]\n",
    "            gold   = ex[\"gold_answer\"]\n",
    "            label  = ex[\"gold_tense\"]\n",
    "\n",
    "            toks = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            logits = model.run_with_saes(\n",
    "                toks.input_ids,\n",
    "                saes=scaled_saes,\n",
    "                use_error_term=None\n",
    "            )\n",
    "            # Greedy next token\n",
    "            next_id = logits[0, -1, :].argmax(dim=-1)\n",
    "            pred    = tokenizer.decode(next_id.unsqueeze(0), skip_special_tokens=True).strip()\n",
    "\n",
    "            counts[label] += 1\n",
    "            if pred and pred[0] == gold:\n",
    "                corrects[label] += 1\n",
    "\n",
    "    # Compute per-tense accuracy\n",
    "    acc = {}\n",
    "    for tense in ['past', 'present', 'future']:\n",
    "        acc[tense] = corrects[tense] / counts.get(tense, 1)\n",
    "    return acc\n",
    "\n",
    "# Load prompts\n",
    "def load_prompts(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "                    \n",
    "\n",
    "# # Run grid search and write to CSV with labeling and thresholding\n",
    "# def run_grid(prompts, output_csv):\n",
    "#     with open(output_csv, 'w', newline='') as csvfile:\n",
    "#         fieldnames = ['label', 'layer', 'feature', 'alpha_amplify', 'accuracy_past', 'accuracy_present', 'accuracy_future']\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         for layer in LAYERS:\n",
    "#             for feature in candidate_features[layer]:\n",
    "#                 for alpha in ALPHAS:\n",
    "#                     acc = evaluate_with_saes(layer, feature, alpha, prompts)\n",
    "#                     valid_labels = [t for t, thr in ACCURACY_THRESHOLD.items() if acc[t] >= thr]\n",
    "#                     if not valid_labels:\n",
    "#                         continue\n",
    "#                     label = '_'.join(sorted(valid_labels))\n",
    "#                     writer.writerow({\n",
    "#                         'label': label,\n",
    "#                         'layer': layer,\n",
    "#                         'feature': feature,\n",
    "#                         'alpha_amplify': alpha,\n",
    "#                         'accuracy_past': acc['past'],\n",
    "#                         'accuracy_present': acc['present'],\n",
    "#                         'accuracy_future': acc['future'],\n",
    "#                     })\n",
    "\n",
    "\n",
    "# Run grid search and write to CSV with labeling and thresholding\n",
    "def run_grid(prompts, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['label','layer','feature','alpha_amplify','accuracy_past','accuracy_present','accuracy_future']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for layer in LAYERS:\n",
    "            for feature in candidate_features[layer]:\n",
    "                for alpha in ALPHAS:\n",
    "                    acc = evaluate_with_saes(layer, feature, alpha, prompts)\n",
    "                    valid = [t for t,thr in ACCURACY_THRESHOLD.items() if acc[t]>=thr]\n",
    "                    if not valid: continue\n",
    "                    for label in valid:\n",
    "                        writer.writerow({\n",
    "                            'label': label,\n",
    "                            'layer': layer,\n",
    "                            'feature': feature,\n",
    "                            'alpha_amplify': alpha,\n",
    "                            'accuracy_past': acc['past'],\n",
    "                            'accuracy_present': acc['present'],\n",
    "                            'accuracy_future': acc['future'],\n",
    "                        })\n",
    "                    \n",
    "# Process grid results: select best alpha per (layer,feature), relabel by thresholds\n",
    "# def process_results(input_csv, output_csv):\n",
    "#     df = pd.read_csv(input_csv)\n",
    "#     # Compute max metric per row\n",
    "#     df['max_acc'] = df[['accuracy_past','accuracy_present','accuracy_future']].max(axis=1)\n",
    "#     # Select best row per layer-feature\n",
    "#     idx = df.groupby(['layer','feature'])['max_acc'].idxmax()\n",
    "#     best = df.loc[idx].copy()\n",
    "#     # Recompute multi-label from thresholds\n",
    "#     def make_label(row):\n",
    "#         labs = [t for t,thr in ACCURACY_THRESHOLD.items() if row[f'accuracy_{t}']>=thr]\n",
    "#         return '_'.join(sorted(labs)) if labs else None\n",
    "#     best['label'] = best.apply(make_label, axis=1)\n",
    "#     best = best.dropna(subset=['label'])\n",
    "#     # Write processed CSV\n",
    "#     best[['layer','label','feature','alpha_amplify']].to_csv(output_csv, index=False)\n",
    "\n",
    "# Process grid results: select best alpha per (layer,feature,label)\n",
    "def process_results(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    rows = []\n",
    "    for label in ['past','present','future']:\n",
    "        df_label = df[df['label'] == label].copy()\n",
    "        df_label['acc'] = df_label[f'accuracy_{label}']\n",
    "        idx = df_label.groupby(['layer','feature'])['acc'].idxmax()\n",
    "        best = df_label.loc[idx]\n",
    "        for _, row in best.iterrows():\n",
    "            rows.append({\n",
    "                'layer': row['layer'],\n",
    "                'label': label,\n",
    "                'feature': row['feature'],\n",
    "                'alpha_amplify': row['alpha_amplify']\n",
    "            })\n",
    "    pd.DataFrame(rows).to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce68489",
   "metadata": {},
   "source": [
    "## Grid search for features, layers, and alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1096882",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prompts = load_prompts(DEV_PROMPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a01642",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid(dev_prompts, \"grid_search_dev_results_translation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad22a1",
   "metadata": {},
   "source": [
    "## Labeling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8f9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process test results\n",
    "process_results(\"grid_search_dev_results_translation.csv\", \"found_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a30bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64604a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a185050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a5260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd360bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42479f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tense_venv)",
   "language": "python",
   "name": "tense_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
