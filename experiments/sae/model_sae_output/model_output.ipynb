{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdc5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7101cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_HOOKS = {\n",
    "    'attention': 'blocks.{layer}.hook_attn_out',\n",
    "    'mlp': 'blocks.{layer}.hook_mlp_out',\n",
    "    'residual': 'blocks.{layer}.hook_resid_post'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a494216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x14ff1f5f6f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args\n",
    "train_csv = \"../all_sentences_train.csv\"\n",
    "temporal_csv = \"../all_sentences_temporal.csv\"\n",
    "hf_token = \"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\"\n",
    "torch_dtype='float16'\n",
    "batch_size = 16\n",
    "model_name = 'meta-llama/Llama-3.1-8B'\n",
    "layers = range(15, 32)\n",
    "out_dir = './model_outputs'\n",
    "n_train = 500\n",
    "n_temporal = 500\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924cc611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dfe7a8134241ab873af00eff6e37a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = getattr(torch, torch_dtype)\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    "    padding_side='left',\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# ensure pad_token exists for batch padding\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map='auto',\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# resize embeddings if pad_token added\n",
    "model_hf.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    model_name,\n",
    "    hf_model=model_hf,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    dtype=dtype\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b24ea",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288d74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, n_per_label):\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    frames = []\n",
    "    for label in ['past', 'present', 'future']:\n",
    "        subset = df[df['tense'] == label].head(n_per_label)\n",
    "        frames.append(subset)\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "def extract_stream(tokenized, verb_indices, hook_name, model):\n",
    "    input_ids = tokenized['input_ids'].to(model.cfg.device)\n",
    "    attention_mask = tokenized['attention_mask'].to(model.cfg.device)\n",
    "    # Use correct signature: tokens tensor and attention_mask keyword\n",
    "#     _, cache = model.run_with_cache(input_ids, attention_mask=attention_mask)\n",
    "    logits, cache = model.run_with_cache(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        names_filter=[hook_name],\n",
    "    )\n",
    "    \n",
    "    acts = cache[hook_name]\n",
    "    out = []\n",
    "    for i, vidx in enumerate(verb_indices):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        positions = [pos for pos, w in enumerate(word_ids) if w == vidx]\n",
    "        if not positions:\n",
    "            raise RuntimeError(f\"No token for verb_index={vidx} in example {i}\")\n",
    "        sub_embs = acts[i, positions, :]\n",
    "        emb = sub_embs.mean(dim=0).cpu().numpy()\n",
    "        out.append(emb)\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "def process_split(df, model, tokenizer, layers, split_name, batch_size, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Prepare dataframes per batch\n",
    "    sentences = df['sentence'].tolist()\n",
    "    verbs = df['verb_index'].tolist()\n",
    "    metadata = df[['language','sentence','main_verb','verb_index','tense']]\n",
    "\n",
    "    for layer in layers:\n",
    "        print(f\"# processing layer {layer}\")\n",
    "        for stream_name, hook_fmt in STREAM_HOOKS.items():\n",
    "            print(f\"## processing stream {stream_name}\")\n",
    "            hook_name = hook_fmt.format(layer=layer)\n",
    "            records = []\n",
    "            # iterate in batches\n",
    "            for start in range(0, len(sentences), batch_size):\n",
    "                batch_sent = sentences[start:start+batch_size]\n",
    "                batch_verbs = verbs[start:start+batch_size]\n",
    "                # tokenize with word alignment\n",
    "                tokenized = tokenizer(\n",
    "                    [s.split() for s in batch_sent],\n",
    "                    is_split_into_words=True,\n",
    "                    return_tensors='pt',\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                # extract activations\n",
    "                emb_batch = extract_stream(tokenized, batch_verbs, hook_name, model)\n",
    "                # collect records\n",
    "                for i, emb in enumerate(emb_batch.numpy()):\n",
    "                    rec = {f'{stream_name}_{j}': float(v)\n",
    "                           for j, v in enumerate(emb)}\n",
    "                    md = metadata.iloc[start + i].to_dict()\n",
    "                    rec.update({'layer': layer, 'stream': stream_name, **md})\n",
    "                    records.append(rec)\n",
    "\n",
    "            df_out = pd.DataFrame.from_records(records)\n",
    "            out_file = os.path.join(\n",
    "                out_dir,\n",
    "                f'llama_{split_name}_layer{layer}_{stream_name}.parquet'\n",
    "            )\n",
    "            df_out.to_parquet(out_file, index=False)\n",
    "            print(f'Saved {out_file}: {df_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e1d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(train_csv, n_per_label=n_train)\n",
    "df_temporal = load_data(temporal_csv, n_per_label=n_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9183fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processing layer 15\n",
      "## processing stream attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.11524708.pbs-m1.metacentrum.cz/ipykernel_823930/2238068270.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./model_outputs/llama_train_layer15_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer15_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer15_residual.parquet: (1500, 4103)\n",
      "# processing layer 16\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer16_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer16_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer16_residual.parquet: (1500, 4103)\n",
      "# processing layer 17\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer17_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer17_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer17_residual.parquet: (1500, 4103)\n",
      "# processing layer 18\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer18_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer18_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer18_residual.parquet: (1500, 4103)\n",
      "# processing layer 19\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer19_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer19_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer19_residual.parquet: (1500, 4103)\n",
      "# processing layer 20\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer20_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer20_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer20_residual.parquet: (1500, 4103)\n",
      "# processing layer 21\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer21_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer21_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer21_residual.parquet: (1500, 4103)\n",
      "# processing layer 22\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer22_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer22_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer22_residual.parquet: (1500, 4103)\n",
      "# processing layer 23\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer23_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer23_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer23_residual.parquet: (1500, 4103)\n",
      "# processing layer 24\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer24_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer24_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer24_residual.parquet: (1500, 4103)\n",
      "# processing layer 25\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer25_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer25_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer25_residual.parquet: (1500, 4103)\n",
      "# processing layer 26\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer26_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer26_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer26_residual.parquet: (1500, 4103)\n",
      "# processing layer 27\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer27_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer27_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer27_residual.parquet: (1500, 4103)\n",
      "# processing layer 28\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer28_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer28_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer28_residual.parquet: (1500, 4103)\n",
      "# processing layer 29\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer29_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer29_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer29_residual.parquet: (1500, 4103)\n",
      "# processing layer 30\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer30_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer30_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer30_residual.parquet: (1500, 4103)\n",
      "# processing layer 31\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer31_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer31_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer31_residual.parquet: (1500, 4103)\n"
     ]
    }
   ],
   "source": [
    "process_split(df_train, model, tokenizer,\n",
    "              layers, 'train',\n",
    "              batch_size, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdd90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processing layer 15\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer15_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer15_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer15_residual.parquet: (1500, 4103)\n",
      "# processing layer 16\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer16_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer16_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer16_residual.parquet: (1500, 4103)\n",
      "# processing layer 17\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer17_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer17_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer17_residual.parquet: (1500, 4103)\n",
      "# processing layer 18\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer18_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer18_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer18_residual.parquet: (1500, 4103)\n",
      "# processing layer 19\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer19_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer19_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer19_residual.parquet: (1500, 4103)\n",
      "# processing layer 20\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer20_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer20_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer20_residual.parquet: (1500, 4103)\n",
      "# processing layer 21\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer21_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer21_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer21_residual.parquet: (1500, 4103)\n",
      "# processing layer 22\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer22_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer22_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer22_residual.parquet: (1500, 4103)\n",
      "# processing layer 23\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer23_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer23_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer23_residual.parquet: (1500, 4103)\n",
      "# processing layer 24\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer24_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer24_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer24_residual.parquet: (1500, 4103)\n",
      "# processing layer 25\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer25_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer25_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer25_residual.parquet: (1500, 4103)\n",
      "# processing layer 26\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer26_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer26_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer26_residual.parquet: (1500, 4103)\n",
      "# processing layer 27\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer27_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer27_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer27_residual.parquet: (1500, 4103)\n",
      "# processing layer 28\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer28_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer28_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer28_residual.parquet: (1500, 4103)\n",
      "# processing layer 29\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer29_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer29_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer29_residual.parquet: (1500, 4103)\n",
      "# processing layer 30\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer30_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer30_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer30_residual.parquet: (1500, 4103)\n",
      "# processing layer 31\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer31_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer31_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer31_residual.parquet: (1500, 4103)\n"
     ]
    }
   ],
   "source": [
    "process_split(df_temporal, model, tokenizer,\n",
    "              layers, 'temporal',\n",
    "              batch_size, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31d4857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processing layer 14\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_train_layer14_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_train_layer14_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_train_layer14_residual.parquet: (1500, 4103)\n"
     ]
    }
   ],
   "source": [
    "process_split(df_train, model, tokenizer,\n",
    "              [14], 'train',\n",
    "              batch_size, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "497ccccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processing layer 14\n",
      "## processing stream attention\n",
      "Saved ./model_outputs/llama_temporal_layer14_attention.parquet: (1500, 4103)\n",
      "## processing stream mlp\n",
      "Saved ./model_outputs/llama_temporal_layer14_mlp.parquet: (1500, 4103)\n",
      "## processing stream residual\n",
      "Saved ./model_outputs/llama_temporal_layer14_residual.parquet: (1500, 4103)\n"
     ]
    }
   ],
   "source": [
    "process_split(df_temporal, model, tokenizer,\n",
    "              [14], 'temporal',\n",
    "              batch_size, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000e00f",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39733f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(train_path, temp_path, eps=1e-6):\n",
    "    # load\n",
    "    df1 = pd.read_parquet(train_path)\n",
    "    df2 = pd.read_parquet(temp_path)\n",
    "\n",
    "    # align on metadata columns\n",
    "    meta_cols = ['language','sentence','main_verb','verb_index','tense','layer','stream']\n",
    "    emb_cols = [c for c in df1.columns if c not in meta_cols]\n",
    "    \n",
    "    # merge on metadata\n",
    "    merged = df1.merge(df2, on=meta_cols, suffixes=('_1','_2'))\n",
    "    # compute difference norms\n",
    "    diffs = merged.apply(\n",
    "        lambda row: np.linalg.norm(\n",
    "            row[[f\"{c}_1\" for c in emb_cols]].values\n",
    "          - row[[f\"{c}_2\" for c in emb_cols]].values\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    # report\n",
    "    n_zero = (diffs < eps).sum()\n",
    "    total = len(diffs)\n",
    "#     print(f\"{n_zero}/{total} embeddings are identical within {eps}\")\n",
    "    if n_zero>0:\n",
    "        identical = merged[diffs<eps][meta_cols]\n",
    "        print(\"Examples with identical embeddings:\")\n",
    "        print(identical.head())\n",
    "    else:\n",
    "        print(\"All embeddings differ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af214877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 15 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 15 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 16 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 16 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 16 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 17 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 17 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 17 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 18 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 18 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 18 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 19 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 19 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 19 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 20 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 20 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 20 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 21 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 21 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 21 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 22 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 22 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 22 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 23 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 23 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 23 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 24 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 24 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 24 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 25 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 25 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 25 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 26 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 26 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 26 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 27 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 27 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 27 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 28 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 28 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 28 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 29 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 29 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 29 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 30 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 30 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 30 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 31 | attention\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 31 | mlp\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n",
      "Layer 31 | residual\n",
      "0/0 embeddings are identical within 1e-06\n",
      "All embeddings differ.\n"
     ]
    }
   ],
   "source": [
    "for layer in range(15,32):\n",
    "        for stream in ['attention','mlp','residual']:\n",
    "            train_f = f'./model_outputs/llama_train_layer{layer}_{stream}.parquet'\n",
    "            temp_f  = f'./model_outputs/llama_temporal_layer{layer}_{stream}.parquet'\n",
    "            print(f\"Layer {layer} | {stream}\")\n",
    "            compare_embeddings(train_f, temp_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b9f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tense_venv)",
   "language": "python",
   "name": "tense_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
