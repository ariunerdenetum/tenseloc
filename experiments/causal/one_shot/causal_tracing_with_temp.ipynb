{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c89f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from itertools import product, chain\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pyvene import (\n",
    "    ConstantSourceIntervention,\n",
    "    LocalistRepresentationIntervention,\n",
    "    IntervenableConfig,\n",
    "    RepresentationConfig,\n",
    "    IntervenableModel,\n",
    "    VanillaIntervention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf08185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90262d5fb2684ec9bb610c0a742eb85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Llama-3.1 8B\n",
    "MODEL = \"meta-llama/Llama-3.1-8b\"\n",
    "HF_TOKEN = \"...\"\n",
    "JSON_FILE = \"translated_prompts_en.json\"\n",
    "M_NOISE   = 10      # number of noise seeds\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\", torch_dtype=torch.float16, token=HF_TOKEN)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f535f",
   "metadata": {},
   "source": [
    "### Factual Recall\n",
    "\n",
    "Let’s set up the model and test it on the fact we want to causal trace: “A cat sat on the mat and he”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c284b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_joint_original(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    device: str,\n",
    "    gold_ids: list[int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Return the joint probability of the gold subtoken sequence `gold_ids`\n",
    "    under `intervenable` (with corruption+restoration), on the prompt.\n",
    "    \"\"\"\n",
    "    gold_joint = 1.0\n",
    "    curr_prompt = prompt\n",
    "\n",
    "    for idx, gold_id in enumerate(gold_ids):\n",
    "        # 1) run the intervened model to get next-token distribution\n",
    "        inputs = tokenizer(curr_prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "        logits = out.logits[0, -1]\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # 2) pick out the probability of the gold subtoken\n",
    "        p_gold = probs[gold_id].item()\n",
    "        gold_joint *= p_gold\n",
    "\n",
    "        # 3) append the decoded gold subtoken to the prompt\n",
    "        subtok = tokenizer.decode([gold_id], clean_up_tokenization_spaces=False)\n",
    "        curr_prompt += subtok\n",
    "\n",
    "    return gold_joint\n",
    "\n",
    "def get_gold_joint_intervened(\n",
    "    intervenable,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    device: str,\n",
    "    sources,\n",
    "    unit_locations,\n",
    "    gold_ids: list[int]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Return the joint probability of the gold subtoken sequence `gold_ids`\n",
    "    under `intervenable` (with corruption+restoration), on the prompt.\n",
    "    \"\"\"\n",
    "    gold_joint = 1.0\n",
    "    curr_prompt = prompt\n",
    "\n",
    "    for idx, gold_id in enumerate(gold_ids):\n",
    "        # 1) run the intervened model to get next-token distribution\n",
    "        inputs = tokenizer(curr_prompt, return_tensors=\"pt\").to(device)\n",
    "        _, out = intervenable(\n",
    "            inputs,\n",
    "            sources,\n",
    "            unit_locations=unit_locations\n",
    "        )\n",
    "        logits = out.logits[0, -1]\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # 2) pick out the probability of the gold subtoken\n",
    "        p_gold = probs[gold_id].item()\n",
    "        gold_joint *= p_gold\n",
    "\n",
    "        # 3) append the decoded gold subtoken to the prompt\n",
    "        subtok = tokenizer.decode([gold_id], clean_up_tokenization_spaces=False)\n",
    "        curr_prompt += subtok\n",
    "\n",
    "    return gold_joint\n",
    "\n",
    "def get_restore_positions(few_shot: str, target: str):\n",
    "    \"\"\"\n",
    "    Returns the list of token indices in few_shot that correspond to\n",
    "    every subtoken of `target`, but only when `target` appears in\n",
    "    the *first* or *last* line of the prompt.\n",
    "    \"\"\"\n",
    "    # 1) Split into lines and record their char‐offset spans\n",
    "    lines = few_shot.split(\"\\n\")\n",
    "#     print(lines)\n",
    "    char_starts = []\n",
    "    cum = 0\n",
    "    for line in lines:\n",
    "        char_starts.append(cum)\n",
    "        cum += len(line) + 1  # +1 for the '\\n'\n",
    "    \n",
    "    first_span = (char_starts[0], char_starts[0] + len(lines[0]))\n",
    "    last_span  = (char_starts[-1], char_starts[-1] + len(lines[-1]))\n",
    "    # ([-1] because last element after split is the empty test prefix line)\n",
    "\n",
    "    # 2) Tokenize with offsets\n",
    "    enc = tokenizer(\n",
    "        few_shot,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    offsets = enc[\"offset_mapping\"]  # list of (start_char, end_char)\n",
    "    token_ids = enc[\"input_ids\"]\n",
    "\n",
    "    # 3) Find all token indices whose span lies fully within first or last line\n",
    "    candidates = []\n",
    "    for i, (st, en) in enumerate(offsets):\n",
    "        in_first = first_span[0] <= st and en <= first_span[1]\n",
    "        in_last  = last_span[0]  <= st and en <= last_span[1]\n",
    "        if not (in_first or in_last):\n",
    "            continue\n",
    "        candidates.append(i)\n",
    "\n",
    "    # 4) Within those, find sub‐ranges matching the target's token IDs\n",
    "    target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "    L = len(target_ids)\n",
    "\n",
    "    pos_restore = []\n",
    "    for i in candidates:\n",
    "        window = token_ids[i:i+L]\n",
    "        if window == target_ids:\n",
    "            pos_restore.extend(range(i+1, i+L+1))\n",
    "\n",
    "    return sorted(set(pos_restore))\n",
    "\n",
    "def get_custom_labels(inputs, pos_restore):\n",
    "    # Prepare positions to intervene\n",
    "    inputs = tokenizer(few_shot, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # Input prompt tokens\n",
    "    temp = []\n",
    "    for i in input_ids:\n",
    "        t = tokenizer.decode(i, clean_up_tokenization_spaces=False)\n",
    "        temp.append(t)\n",
    "    # Save for visualization\n",
    "    custom_labels = list(temp)\n",
    "    # mark restored tokens\n",
    "    for idx in pos_restore:\n",
    "        custom_labels[idx] += \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af5637",
   "metadata": {},
   "source": [
    "### Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c08cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the noise intervention class\n",
    "class NoiseIntervention(ConstantSourceIntervention, LocalistRepresentationIntervention):\n",
    "    def __init__(self, embed_dim=None, seed: int = 1, **kwargs):\n",
    "        super().__init__()\n",
    "        # pull the provided size\n",
    "        embed_dim = embed_dim or kwargs.get(\"latent_dim\")\n",
    "        if embed_dim is None:\n",
    "            raise ValueError(f\"No latent_dim in kwargs: {list(kwargs)}\")\n",
    "        self.interchange_dim = embed_dim\n",
    "\n",
    "        rs = np.random.RandomState(seed)\n",
    "        prng = lambda *shape: rs.randn(*shape)\n",
    "        self.noise = torch.from_numpy(prng(1, 1, embed_dim)).to(device)\n",
    "        self.noise_level = 0.13462981581687927\n",
    "\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        base[..., :self.interchange_dim] += self.noise * self.noise_level\n",
    "        return base\n",
    "    \n",
    "# 2) Build a corrupt config for Llama-3.1 8B\n",
    "def corrupted_config(model_type, layer, seed: int = 1):\n",
    "    return IntervenableConfig(\n",
    "        model_type=model_type,\n",
    "        representations=[\n",
    "            RepresentationConfig(\n",
    "                layer,\n",
    "                \"block_input\"\n",
    "            ),\n",
    "        ],\n",
    "        intervention_types=NoiseIntervention,\n",
    "        intervention_additional_kwargs=[ {\"seed\": seed} ]\n",
    "    )\n",
    "\n",
    "def restore_corrupted_with_interval_config(\n",
    "    intervened_layer: int,\n",
    "    restore_layer: int,\n",
    "    stream: str = \"mlp\",\n",
    "    window: int = 3,\n",
    "    num_layers: int = 33,\n",
    "    seed: int = 1\n",
    "):\n",
    "    # compute restore interval around restore_layer\n",
    "    half = window // 2\n",
    "    start = max(0, restore_layer - half)\n",
    "    end   = min(num_layers, restore_layer + half + 1)\n",
    "    \n",
    "    reps = []\n",
    "    types = []\n",
    "    kwargs_list = [ {\"seed\": seed} ]\n",
    "    \n",
    "    # First: corrupt the embedding of token position later\n",
    "    reps.append(\n",
    "      RepresentationConfig(\n",
    "        0,\n",
    "        \"block_input\"\n",
    "      )\n",
    "    )\n",
    "    types.append(NoiseIntervention)\n",
    "    \n",
    "    # Then: restore hidden state at each layer in [start, end)\n",
    "    for L in range(start, end):\n",
    "        reps.append(\n",
    "          RepresentationConfig(\n",
    "            L,\n",
    "            stream \n",
    "          )\n",
    "        )\n",
    "        types.append(VanillaIntervention)\n",
    "        kwargs_list.append({})\n",
    "    \n",
    "    return IntervenableConfig(\n",
    "      model_type=type(model),\n",
    "      representations=reps,\n",
    "      intervention_types=types,\n",
    "      intervention_additional_kwargs=kwargs_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d00921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE = \"translated_prompts_en.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75813311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on seed 0...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 1...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 2...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 3...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 4...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 5...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n",
      "Saved\n",
      "Working on seed 6...\n",
      "Working on stream block_output...\n",
      "Working on stream attention_output...\n",
      "Working on stream mlp_activation...\n"
     ]
    }
   ],
   "source": [
    "# 5) Create intervenable wrapper for a given layer\n",
    "layer = 0\n",
    "\n",
    "df = pd.read_json(JSON_FILE)\n",
    "streams = [\"block_output\", \"attention_output\", \"mlp_activation\"]\n",
    "\n",
    "for entry in df.itertuples():\n",
    "    pid, prompt, restore_words, gold = entry.prompt_id, entry.prompt, entry.words_restore, entry.gold\n",
    "    # parse language & tense\n",
    "    tense, lang = pid.split(\"_\")\n",
    "    lang = lang[:2]\n",
    "\n",
    "    # compute clean baseline once\n",
    "    gold_ids = tokenizer.encode(f\" {gold}\", add_special_tokens=False)\n",
    "    p_clean  = get_gold_joint_original(model, tokenizer, prompt, device, gold_ids)\n",
    "\n",
    "    # compute restore positions once\n",
    "    pos_restore = sorted(set(chain.from_iterable(\n",
    "        get_restore_positions(prompt, w) for w in restore_words\n",
    "    )))\n",
    "    \n",
    "    base = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    for seed in range(M_NOISE):\n",
    "        rows = []\n",
    "        print(f\"Working on seed {seed}...\")\n",
    "        # (re)set up noise intervention with new random seed\n",
    "        config = corrupted_config(type(model), layer=0, seed=seed)\n",
    "        intervenable  = IntervenableModel(config, model)\n",
    "\n",
    "        # corrupt only baseline\n",
    "        p_corrupt = get_gold_joint_intervened(\n",
    "            intervenable, tokenizer, prompt, device,\n",
    "            sources=None,\n",
    "            unit_locations={\"base\":[[pos_restore]]},\n",
    "            gold_ids=gold_ids\n",
    "        )\n",
    "        \n",
    "        for stream in streams:\n",
    "            print(f\"Working on stream {stream}...\")\n",
    "            for restore_layer in range(33):\n",
    "                for pos in range(base.input_ids.size(1)):\n",
    "                    cfg = restore_corrupted_with_interval_config(\n",
    "                        intervened_layer=0,      # we only corrupt at input\n",
    "                        restore_layer=restore_layer,\n",
    "                        stream=stream,\n",
    "                        window=3,\n",
    "                        num_layers=model.config.num_hidden_layers,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                    interv = IntervenableModel(cfg, model)\n",
    "\n",
    "                    # First prediction\n",
    "                    n_restores = len(cfg.representations) - 1\n",
    "                    sources = [None] + [base]*n_restores\n",
    "                    unit_locations = {\n",
    "                        \"sources->base\": (\n",
    "                            [None] + [[[pos]]]*n_restores,\n",
    "                            [[pos_restore]] + [[[pos]]]*n_restores,\n",
    "                        )\n",
    "                    }\n",
    "\n",
    "                    p_restored = get_gold_joint_intervened(\n",
    "                        intervenable=interv,\n",
    "                        tokenizer=tokenizer,\n",
    "                        prompt=prompt,\n",
    "                        device=device,\n",
    "                        sources=sources,\n",
    "                        unit_locations=unit_locations,\n",
    "                        gold_ids=gold_ids,\n",
    "                    )\n",
    "                    \n",
    "                    rows.append({\n",
    "                        \"language\":        lang,\n",
    "                        \"tense\":           tense,\n",
    "                        \"stream\":          stream,\n",
    "                        \"prompt_id\":       pid,\n",
    "                        \"pos\":             pos,\n",
    "                        \"noise_seed\":      seed,\n",
    "                        \"restore_layer\":   restore_layer,\n",
    "                        \"gold\":            gold,\n",
    "                        \"p_clean\":         p_clean,\n",
    "                        \"p_corrupt\":       p_corrupt,\n",
    "                        \"p_restored\":      p_restored,\n",
    "                        \"delta_corrupt\":   p_clean - p_corrupt,\n",
    "                        \"delta_restored\":  p_restored - p_corrupt\n",
    "                    })\n",
    "\n",
    "        pd.DataFrame(rows).to_csv(f\"{lang}_{seed}_{pid}.csv\", index=False)\n",
    "        print(f\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f716d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tense_venv)",
   "language": "python",
   "name": "tense_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
